{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Final Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of Chess Game Outcomes Based on Player Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "Hopefully your team is at least this good.\n",
    "\n",
    "- Omri Habot\n",
    "- Keate Ehrenburg\n",
    "- Jacob Lamadrid\n",
    "- Alex Bumbalov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "The goal of this project is to examine the effectiveness of different models in predicting the outcome of a chess match. The data used in this project will contain features describing players’ technical skill and experience including the Elo rating of each player, the number of games they have played, the number of wins/losses/draws they have achieved, their age, and the length of the game. These are measured by observing the outcomes of every game in a player’s career. We will use this data to perform feature selection and single out the most relevant features, and then we will train the models and compare their predictive accuracies. Accuracy will be measured by classification error metrics including precision, recall, f1-score, and possibly others, which we will compare across models to observe their relative efficacies."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Within the world of chess competitions and general play, machine learning and deep learning have been famously applied in such algorithms as Deep Blue, Stockfish, etc. These algorithms historically have aimed to predict the next best move to be executed as well as the win probability at any given position state [1](https://arxiv.org/pdf/2109.11602.pdf). This win probability is what we aim to place our project emphasis upon as in similar works in varying competitive settings, most relevant of which is found in esports win/loss classification based upon player/team rankings and typical movesets among many other features [2](https://arxiv.org/pdf/2108.02799.pdf). The application of these algorithms in returning win probability and outcome prediction has large impacts in the way one chooses to learn chess or the way in which a machine is taught how to play chess. This may also have implications for the way in which new strategies or entire play styles are formed. These implications have already manifested themselves in the competitive playspace as many players look towards Stockfish evaluations for input on their play or for other engines in which competition may aid in their training as “with the help of chess engines, Grandmasters are now able to plan prepare for their games in extreme depth, sometimes memorizing up to 15-20 moves of their openings” [3](https://fluency.mcsaatchi.com/2022/09/01/the-evolution-of-chess-ai/#:~:text=Artificial%20intelligence%20has%20completely%20changed,20%20moves%20of%20their%20openings.). In order to approach the problem of finding an appropriate machine learning prediction method , we are going to need a large amount of chess match data to work with. We have chosen to use the Free Internet Chess Server(FICS) database. The database has been collecting all rated and unrated games played on FICS since October of 2008. With over 200 million games stored, this will provide us with plenty of data to address our goal of outcome classification.\n",
    "\n",
    "Our models will train similarly to current chess engines, utilizing previous games played in order to gain improve decision making and outcome prediction, as in the models created by the chess.com website, with machine learning algorithms such as Luk.AI which improves its elo with every game played."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The problem we will be addressing is determining which machine learning method best predicts the winner of a chess game depending on variables like participating players, player statistics and scores such as their Elo’s, the opening, and psychological factors of the players, among others. We plan to explore this problem by trying three different machine learning algorithms: logistic regression, random forest, and deep neural network. Logistic regression is a solid classification method. While we fear that it may be too simple for this task, we believe it is a simple baseline to compare with other, more complex models. We plan to try random forests because the complex decision boundaries may be sufficient for consistently accurate and generalizable predictions. Finally, we plan on using a deep neural network because it seems to be the baseline method used for very similar problems, and given our large dataset, we hope it will pick up on important nuances that are neglected by logistic regression and random forest. We will use labeled chess game data that denotes the winner of each game as well as the aforementioned variables that describe the game. Each model will be trained using the same (or similar) dataset and evaluated with the same set of classification error metrics such that we can compare their efficacies. These metrics include precision, recall, f1-score, and others."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We will be using the [Free Internet Chess Server (FICS)](https://www.ficsgames.org/dl/ficsgamesdb_202301_blitz2000_movetimes_278527.pgn.bz2) as our database for analysis. The dataset comes in the Portable Game Notation (PGN) file format, which is a common data format for representing chess games. We will need to convert this into a pandas dataframe for our analysis, so we will write a function that parses through the PGN file as text and builds a dataframe from it. Once the data is parsed, we will simply need to convert each column into the appropriate datatype, so numbers would be int32 or float64, words will be strings, binary features will be boolean, etc.\n",
    "\n",
    "There are many different types of chess games from which we can choose that would cause nuances in our analysis and results. We selected blitz games because their rapid nature produces more variability in game moves, and therefore outcomes. We hope that this variability gives us more data to work with when predicting outcomes. Our dataset has 25884 complete observations (each representing a chess game) as well as 22 features describing each game (before one-hot encoding). Each observation consists of the moves made throughout the game, the date and time played, the players and their colors, player Elo, match result, and game variant and time control information for each match. One of the most critical features is `playerElo`, a score representing the relative skill levels of players, since this is likely one of the biggest predictors of the outcome. This is represented by an arbitrary number that can indicate a player's relative skill level when compared to other players. Another crucial feature is `moves`, which outlines all the moves made in the game. We can use this sequential data to build more complex models and make more nuanced predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Importing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database provides chess game data in the PGN file format. In order to convert the data into a dataframe and proceed with our analysis, we wrote scripts to parse our desired variables from PGN files and produce CSV files. Afterwards, we imported the CSV's as dataframes and cleaned them up such that each feature had the proper datatype. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# import all csv's from a folder and combine them into one dataframe\n",
    "def combine_data_files(folder_path):\n",
    "    df_list = []\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "            df_list.append(df)\n",
    "\n",
    "    return pd.concat(df_list, axis=0, ignore_index=True)\n",
    "\n",
    "# clean up the datatypes of the imported dataframes\n",
    "def clean_df_new(df):\n",
    "    \n",
    "    resMap = lambda res : 'White' if res == '1-0' else 'Black' if res == '0-1' else 'Draw' if res == \"1/2-1/2\" else res\n",
    "    cleanDateTime = lambda d: pd.to_datetime(d)\n",
    "    cleanComment = lambda x: x if x.find(']') == -1 else x.split(\"] \")[1]\n",
    "\n",
    "    datetime_columns = ['Date', 'Time', 'WhiteClock', 'BlackClock']\n",
    "    float_columns = ['WhiteRD', 'BlackRD', 'AvgEvalOpening', 'AvgEvalMiddle', 'AvgEvalEnd', 'AvgEmtOpening', 'AvgEmtMiddle', 'AvgEmtEnd']\n",
    "    int_columns = ['FICSGamesDBGameNo', 'WhiteElo', 'BlackElo', 'PlyCount']\n",
    "\n",
    "    df = df.drop(['Event', 'Site', 'Round', 'WhiteClock', 'BlackClock'], axis=1)\n",
    "\n",
    "    for column in df.columns:\n",
    "\n",
    "        if column in float_columns:\n",
    "            df[column] = df[column].astype(float)\n",
    "        elif column in int_columns:\n",
    "            df[column] = df[column].astype(int)\n",
    "        elif column in datetime_columns:\n",
    "            df[column] = df[column].apply(cleanDateTime)\n",
    "        else:\n",
    "            if column == 'Result':\n",
    "                df[column] = df[column].apply(resMap)\n",
    "            if column == 'ResultComment':\n",
    "                df[column] = df[column].apply(cleanComment)\n",
    "            df[column] = df[column].astype('category')\n",
    "\n",
    "    df['WhiteIsComp'] = df['WhiteIsComp'] == 'Yes'\n",
    "    df['BlackIsComp'] = df['BlackIsComp'] == 'Yes'\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25884, 22)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = combine_data_files('data-by-month')\n",
    "cleaned = clean_df_new(combined_df).dropna()\n",
    "cleaned.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the datatypes, we performed the last processing step: applying one-hot encoding to categorical features and converting redundant numerical features into averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional processing: perform one-hot encoding and take average of given numerical columns\n",
    "def construct_df_train(df):\n",
    "    # Define columns to take the average of and to one-hot encode\n",
    "    cols_to_avg = ['WhiteRD', 'BlackRD', 'AvgEvalOpening', 'AvgEvalMiddle', 'AvgEvalEnd', 'AvgEmtOpening', 'AvgEmtMiddle', 'AvgEmtEnd', 'PlyCount']\n",
    "    cols_to_onehot = ['ECO'] # 'White', 'Black',  <--- For if we decide to train using player UN for black and white as a features\n",
    "    ground_truth_labels = ['Result', 'ResultComment']\n",
    "\n",
    "    # Define dictionary to map column names to letters for one-hot encoding\n",
    "    col_name_to_letter = {col_name: chr(65+i) for i, col_name in enumerate(ground_truth_labels+cols_to_onehot)}\n",
    "\n",
    "    # Define column names for player-wise averages\n",
    "    new_cols_white = ['WhiteAvgRD', 'WhiteAvgEvalForOpenings', 'WhiteAvgEvalForMiddlegames', 'WhiteAvgEvalForEndgames', 'WhiteAvgEmtForOpenings', 'WhiteAvgEmtForMiddlegames', 'WhiteAvgEmtForEndgames', 'WhiteAvgPlyCount']\n",
    "    new_cols_black = ['BlackAvgRD', 'BlackAvgEvalForOpenings', 'BlackAvgEvalForMiddlegames', 'BlackAvgEvalForEndgames', 'BlackAvgEmtForOpenings', 'BlackAvgEmtForMiddlegames', 'BlackAvgEmtForEndgames', 'BlackAvgPlyCount']\n",
    "\n",
    "    # Compute player-wise averages\n",
    "    dataAvgsByUserWhite = {p: df[df['White'] == p][cols_to_avg+cols_to_onehot].mean(numeric_only=True) for p in df['White'].unique()}\n",
    "    dataAvgsByUserBlack = {p: df[df['Black'] == p][cols_to_avg+cols_to_onehot].mean(numeric_only=True) for p in df['Black'].unique()}\n",
    "\n",
    "    # Define empty dataframe to hold the new data\n",
    "    df_new = pd.DataFrame(columns = ground_truth_labels+cols_to_onehot+new_cols_white+new_cols_black)\n",
    "\n",
    "    # Iterate over each row in the original dataframe and create a new row in the output dataframe with the necessary data\n",
    "    for row in df.itertuples():\n",
    "        white_avg_values = dataAvgsByUserWhite.get(row.White, pd.Series([0]*len(cols_to_avg+cols_to_onehot)))\n",
    "        black_avg_values = dataAvgsByUserBlack.get(row.Black, pd.Series([0]*len(cols_to_avg+cols_to_onehot)))\n",
    "\n",
    "        white_new_cols = [white_avg_values[0], white_avg_values[2], white_avg_values[3], white_avg_values[4], white_avg_values[5], white_avg_values[6], white_avg_values[7], white_avg_values[8]]\n",
    "        black_new_cols = [black_avg_values[1], black_avg_values[2], black_avg_values[3], black_avg_values[4], black_avg_values[5], black_avg_values[6], black_avg_values[7], black_avg_values[8]]\n",
    "\n",
    "        new_row = tuple(getattr(row, label) for label in ground_truth_labels+cols_to_onehot) + tuple(white_new_cols) + tuple(black_new_cols)\n",
    "\n",
    "        df_new.loc[len(df_new)] = new_row\n",
    "\n",
    "    # One-hot encode the ground truth labels and cols_to_onehot\n",
    "    df_new = pd.get_dummies(df_new, columns = ground_truth_labels+cols_to_onehot, prefix = list(col_name_to_letter.values()))\n",
    "\n",
    "    # Map the column labels to their corresponding uppercase letters\n",
    "    df_new.columns = df_new.columns.map(lambda x: col_name_to_letter.get(x, x))\n",
    "\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations: 25884\n",
      "Number of features (after one-hot encoding): 517\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WhiteAvgRD</th>\n",
       "      <th>WhiteAvgEvalForOpenings</th>\n",
       "      <th>WhiteAvgEvalForMiddlegames</th>\n",
       "      <th>WhiteAvgEvalForEndgames</th>\n",
       "      <th>WhiteAvgEmtForOpenings</th>\n",
       "      <th>WhiteAvgEmtForMiddlegames</th>\n",
       "      <th>WhiteAvgEmtForEndgames</th>\n",
       "      <th>WhiteAvgPlyCount</th>\n",
       "      <th>BlackAvgRD</th>\n",
       "      <th>BlackAvgEvalForOpenings</th>\n",
       "      <th>...</th>\n",
       "      <th>C_E87</th>\n",
       "      <th>C_E88</th>\n",
       "      <th>C_E90</th>\n",
       "      <th>C_E91</th>\n",
       "      <th>C_E92</th>\n",
       "      <th>C_E93</th>\n",
       "      <th>C_E94</th>\n",
       "      <th>C_E95</th>\n",
       "      <th>C_E96</th>\n",
       "      <th>C_E97</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.348602</td>\n",
       "      <td>23.614612</td>\n",
       "      <td>16.099574</td>\n",
       "      <td>28.836774</td>\n",
       "      <td>1.897313</td>\n",
       "      <td>2.309180</td>\n",
       "      <td>1.631107</td>\n",
       "      <td>160.004658</td>\n",
       "      <td>35.060330</td>\n",
       "      <td>-4.545055</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34.851760</td>\n",
       "      <td>54.018170</td>\n",
       "      <td>128.992509</td>\n",
       "      <td>265.426934</td>\n",
       "      <td>1.435165</td>\n",
       "      <td>2.490632</td>\n",
       "      <td>2.255974</td>\n",
       "      <td>101.928292</td>\n",
       "      <td>30.245794</td>\n",
       "      <td>24.857252</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.075718</td>\n",
       "      <td>45.014569</td>\n",
       "      <td>104.508098</td>\n",
       "      <td>195.642712</td>\n",
       "      <td>2.104136</td>\n",
       "      <td>3.775255</td>\n",
       "      <td>3.686790</td>\n",
       "      <td>92.955514</td>\n",
       "      <td>30.245794</td>\n",
       "      <td>24.857252</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49.572727</td>\n",
       "      <td>60.490440</td>\n",
       "      <td>140.078384</td>\n",
       "      <td>192.691169</td>\n",
       "      <td>1.725352</td>\n",
       "      <td>3.174036</td>\n",
       "      <td>2.597224</td>\n",
       "      <td>89.421488</td>\n",
       "      <td>34.200000</td>\n",
       "      <td>52.580662</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22.756332</td>\n",
       "      <td>8.380797</td>\n",
       "      <td>-52.832145</td>\n",
       "      <td>-53.019966</td>\n",
       "      <td>0.547576</td>\n",
       "      <td>1.102669</td>\n",
       "      <td>1.044522</td>\n",
       "      <td>81.209607</td>\n",
       "      <td>33.835407</td>\n",
       "      <td>29.418740</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 517 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   WhiteAvgRD  WhiteAvgEvalForOpenings  WhiteAvgEvalForMiddlegames  \\\n",
       "0   29.348602                23.614612                   16.099574   \n",
       "1   34.851760                54.018170                  128.992509   \n",
       "2   34.075718                45.014569                  104.508098   \n",
       "3   49.572727                60.490440                  140.078384   \n",
       "4   22.756332                 8.380797                  -52.832145   \n",
       "\n",
       "   WhiteAvgEvalForEndgames  WhiteAvgEmtForOpenings  WhiteAvgEmtForMiddlegames  \\\n",
       "0                28.836774                1.897313                   2.309180   \n",
       "1               265.426934                1.435165                   2.490632   \n",
       "2               195.642712                2.104136                   3.775255   \n",
       "3               192.691169                1.725352                   3.174036   \n",
       "4               -53.019966                0.547576                   1.102669   \n",
       "\n",
       "   WhiteAvgEmtForEndgames  WhiteAvgPlyCount  BlackAvgRD  \\\n",
       "0                1.631107        160.004658   35.060330   \n",
       "1                2.255974        101.928292   30.245794   \n",
       "2                3.686790         92.955514   30.245794   \n",
       "3                2.597224         89.421488   34.200000   \n",
       "4                1.044522         81.209607   33.835407   \n",
       "\n",
       "   BlackAvgEvalForOpenings  ...  C_E87  C_E88  C_E90  C_E91  C_E92  C_E93  \\\n",
       "0                -4.545055  ...      0      0      0      0      0      0   \n",
       "1                24.857252  ...      0      0      0      0      0      0   \n",
       "2                24.857252  ...      0      0      0      0      0      0   \n",
       "3                52.580662  ...      0      0      0      0      0      0   \n",
       "4                29.418740  ...      0      0      0      0      0      0   \n",
       "\n",
       "   C_E94  C_E95  C_E96  C_E97  \n",
       "0      0      0      0      0  \n",
       "1      0      0      0      0  \n",
       "2      0      0      0      0  \n",
       "3      0      0      0      0  \n",
       "4      0      0      0      0  \n",
       "\n",
       "[5 rows x 517 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = construct_df_train(cleaned)\n",
    "print('Number of observations: ' + str(df_train.shape[0]))\n",
    "print('Number of features (after one-hot encoding): ' + str(df_train.shape[1]))\n",
    "df_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "In this project we aim to evaluate the effectiveness of three different machine learning implementations at classifying the outcomes of FIDE titled chess games from unlabeled match data. Each model (Logisitc Regression, Random Forests, and the Deep NN) will be trained on a curated sample database consisting of several derived features representing characteristics of the games pertinent to the games’ outcomes. We chose logistic regression as our first model because it is a solid classification method. While we fear that it may be too simple for this task, we believe it is a good, simple baseline to compare with more complex models. We also plan to try random forests because the complex decision boundaries may be sufficient for consistently accurate and generalizable predictions. Finally, we will use a deep neural network because it seems to be the baseline method used for very similar problems regarding chess game outcomes, and given our large dataset, we hope it will pick up on important nuances that are neglected by logistic regression and random forest. \n",
    "\n",
    "For this we will need to use a variety of tools for accessing and manipulating the data to generate these special features. Pandas and numpy will be instrumental in this preprocessing stage. Further imports will be required for sklearn and matplot lib for training/validation and analysis, as well as an additional library called pychess will be necessary for processing chess engine ratings and to help format game data for preprocessing. We will be using built in methods for training and validating data provided by sklearn. To measure the effectiveness of our model we will take Sayon Bhattacharjee’s LSTM solution published in the _Towards Data Science_ article found [here](https://towardsdatascience.com/predicting-professional-players-chess-moves-with-deep-learning-9de6e305109e). Sayon's LTSM solution uses two convolutional neural networks; one \"predicts\" where a particular piece moved from, and the other predicts where a piece will move. These networks are combined as a time series to model a classification problem with three possible output categories corresponding with the outcome: black wins, white wins, or there is a draw. The effectiveness of this model can simply be measured by classification accuracy using metrics discussed in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "In the evaluation of binary chess outcome classification, simply implementing a confusion matrix of our algorithm’s results is most appropriate for visualizing the performance. In this situation, there is equal cost associated with both false positive rates and false negative rates, and there is equal cost associated with true positives and true negative rates. Consequently, we care about both the recall and precision of our models equally. This suggests that an f1-score is one primary metric we should use, since it is equally sensitive to both recall and precision. For example, predicting that Player 1 wins when they actually win is equally as important as being ‘accurate’ when predicting that Player 1 will win; the cost of a false positive is no different than the cost of a false negative. We may also explore some other classification error metrics such as likelihood ratios.\n",
    "\n",
    "$$precision = \\frac{TP}{TP + FP}$$\n",
    "$$recall = \\frac{TP}{TP + FN}$$\n",
    "$$F1-Score = \\frac{2*precision*recall}{precision + recall}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "# __NEEDS WORK__\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Maybe you include a learning curve to show whether you have enough data to do train/validate/test split or have to go to k-folds or LOOCV or ???\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "# __NEEDS WORK__\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "### Limitations\n",
    "# __NEEDS WORK__\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "In the case of data privacy, we see no major concerns with our data as it is publicly available FIDE match information that is made public anytime a chess player agrees to a FIDE match. The biggest ethical concern that we could potentially see with the production of any well-functioning model for predicting FIDE titled chess matches would be the use of the model for gambling purposes. Although we don't foresee the prediction accuracy of our models being able to give a definitive answer to who will win a match, a good model could lead to an increase in chess match gambling which comes with its own host of harmful consequences. Additionally, we can highlight the concern that a prediction model could affect the way that players go about their matches if they know the predicted outcome of a game given specific features. This could be seen as an unfair advantage. The model, if then re-trained with new data, could also perform much more poorly as outcomes would now be affected by prior knowledge of the potential outcome of a game which would complicate further predictions.\n",
    "\n",
    "### Conclusion\n",
    "# __NEEDS WORK__\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
